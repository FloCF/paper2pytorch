{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJzhKAiA1C-4"
   },
   "source": [
    "# [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "### Jun-Yan Zhu, Taesung Park, Phillip Isola & Alexei A. Efros\n",
    "\n",
    "Algorithm for unpaired image-to-image translation.\n",
    "\n",
    "It consists of two CNNs $G$ and $F$, where $G$ maps the image $X$ to the domain $D_Y$ and $F$ maps $Y$ back to the domain $D_X$. The cyclical consitency error then forces the CNNs to produce images where input image $X$ and projected image $F(G(X))=\\hat{x}$ as well as input $Y$ and projected $G(F(Y))=\\hat{y}$ are pixelwise close to eachother. The graph below from the original paper cited above illustrates the situation.\n",
    "\n",
    "\n",
    "<img src=\"images/cycleGAN-formulation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Hy6xCF-lJAf"
   },
   "source": [
    "## Set Up the cycleGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTnRx5NK1C-_"
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from random import sample\n",
    "\n",
    "import os\n",
    "\n",
    "# Check for CUDA device\n",
    "device_txt = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_txt)\n",
    "# Print Device Type\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "\n",
    "### Define by User ###\n",
    "# Choose dataset, available [\"ae_photos\",\"apple2orange\", \"cezanne2photo\", \"cityscapes\", \"facades\",\n",
    "#                            \"horse2zebra\", \"iphone2dslr_flower\", \"maps\", \"mini\", \"mini_colorization\",\n",
    "#                            \"mini_pix2pix\", \"monet2photo\", \"summer2winter_yosemite\", \"ukiyoe2photo\",\n",
    "#                            \"vangogh2photo\"]\n",
    "use_dataset = 'horse2zebra'\n",
    "######################\n",
    "# Define Image size\n",
    "img_size = 128 if use_dataset==\"cityscapes\" else 256\n",
    "\n",
    "# Define Training Details\n",
    "epochs     = 200\n",
    "batch_size = 1\n",
    "# Learning Rate (Adam) and beta1 (beta2=0.999 always)\n",
    "lr    = 0.0002\n",
    "beta1 = 0.5\n",
    "# Epoch when starting with linear LR decay\n",
    "epoch_lr_decline = 100\n",
    "# Loss functions lambdas\n",
    "## Cyclincal Loss\n",
    "cyc_lamb = 10.\n",
    "## Identity Loss for paintings\n",
    "ident_lamb = .5*cyc_lamb if use_dataset in [\"monet2photo\", \"iphone2dslr_flower\"] else 0.\n",
    "lambs = (cyc_lamb, ident_lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_b4f6lhulJB1"
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKIK5m5PlJCE"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Download dataset\n",
    "\n",
    "url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/'+use_dataset+'.zip'\n",
    "urllib.request.urlretrieve(url, './data.zip')\n",
    "# Make folders, unzip and delete file\n",
    "!mkdir data saved_models\n",
    "!unzip -qq -o data.zip -d data/ && rm data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yudMFKeU1C_f"
   },
   "source": [
    "## Define the two Generator Networks\n",
    "\n",
    "Following the archetecture of transformation net form [Johnson et al.](https://cs.stanford.edu/people/jcjohns/eccv16/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wFaxcTF1C_l"
   },
   "outputs": [],
   "source": [
    "class Basic_Layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, norm_layer, use_relu=True):\n",
    "        super(Basic_Layer, self).__init__()\n",
    "        self.use_relu = use_relu\n",
    "        self.pad  = nn.ReflectionPad2d(kernel_size // 2)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride)\n",
    "        self.norm = norm_layer(out_ch)\n",
    "       \n",
    "    def forward(self, input):\n",
    "        input = self.pad(input)\n",
    "        if self.use_relu:\n",
    "            out = F.relu(self.norm(self.conv(input)), inplace=True)\n",
    "        else:\n",
    "            out = self.norm(self.conv(input))\n",
    "        return out\n",
    "        \n",
    "class Res_Block(nn.Module):\n",
    "    def __init__(self, n_ch, norm_layer):\n",
    "        super(Res_Block, self).__init__()\n",
    "        self.layer1 = Basic_Layer(n_ch, n_ch, kernel_size=3, stride=1, norm_layer=norm_layer)\n",
    "        self.layer2 = Basic_Layer(n_ch, n_ch, kernel_size=3, stride=1, norm_layer=norm_layer,\n",
    "                                  use_relu=False)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        identity = input \n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input) \n",
    "        out   = input + identity\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, norm_layer):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_ch, out_ch, kernel_size, 2, padding=kernel_size // 2, output_padding=1)\n",
    "        self.norm   = norm_layer(out_ch)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        out = F.relu(self.norm(self.deconv(input)), inplace=True)\n",
    "        return out\n",
    "\n",
    "class G_net(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, ngf=64, n_res_blocks=6, norm_layer=nn.InstanceNorm2d):\n",
    "        super(G_net, self).__init__()\n",
    "        # Define Encoding layers\n",
    "        self.enco1  = Basic_Layer(in_nc, ngf, kernel_size=7, stride=1, norm_layer=norm_layer)\n",
    "        self.enco2  = Basic_Layer(ngf, ngf*2, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        self.enco3  = Basic_Layer(ngf*2, ngf*4, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        # Define Residual layers\n",
    "        self.residual = nn.Sequential(*[Res_Block(ngf*4, norm_layer=norm_layer)]*n_res_blocks)\n",
    "        # Define Decoding layers\n",
    "        self.deco1  = Upsample(ngf*4, ngf*2, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        self.deco2  = Upsample(ngf*2, ngf, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        self.deco3  = nn.Conv2d(ngf, out_nc, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Encoding\n",
    "        input = self.enco1(input)\n",
    "        input = self.enco2(input)\n",
    "        input = self.enco3(input)\n",
    "        # Residual\n",
    "        input = self.residual(input)\n",
    "        # Decoding\n",
    "        input = self.deco1(input)\n",
    "        input = self.deco2(input)\n",
    "        input = self.deco3(input)\n",
    "        return torch.tanh(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmQEos_e1C_t"
   },
   "source": [
    "### Define Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFUXLDI51C_v"
   },
   "outputs": [],
   "source": [
    "# Define 70x70 PatchGAN Discriminator\n",
    "class D_patch(nn.Module):\n",
    "    def __init__(self, in_nc, ndf=64, norm_layer = nn.InstanceNorm2d):\n",
    "        super(D_patch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_nc, ndf, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm2 = norm_layer(ndf*2)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm3 = norm_layer(ndf*4)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm4 = norm_layer(ndf*8)\n",
    "        self.final = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = F.leaky_relu(self.conv1(input), negative_slope=0.2, inplace=True)\n",
    "        input = F.leaky_relu(self.norm2(self.conv2(input)), negative_slope=0.2, inplace=True)\n",
    "        input = F.leaky_relu(self.norm3(self.conv3(input)), negative_slope=0.2, inplace=True)\n",
    "        input = F.leaky_relu(self.norm4(self.conv4(input)), negative_slope=0.2, inplace=True)\n",
    "        return torch.sigmoid(self.final(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wlm8PKM51C_1"
   },
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qT48CEma1C_3"
   },
   "outputs": [],
   "source": [
    "# custom weights initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight, mean=1.0, std=0.02)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEgWqjaI1DAN"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZalYGkd1DAR"
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(Dataset):\n",
    "    def __init__(self, root, mode='train', unaligned=True, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to folders for train, val amd test\n",
    "            mode (str): Either 'train' or 'test'\n",
    "            direction (str): Either 'AtoB' or 'BtoA' indicating which direction the prediction should go\n",
    "            unaligned (bool): If unpaired or paired dataset\n",
    "            transform (torchvision obj) : Usual image preprocessing\n",
    "        \"\"\"\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.files_A   = [os.path.join(root, '%sA/' % mode) + x for x in sorted(os.listdir(os.path.join(root, '%sA/' % mode)))]\n",
    "        self.files_B   = [os.path.join(root, '%sB/' % mode) + x for x in sorted(os.listdir(os.path.join(root, '%sB/' % mode)))]\n",
    "        self.transform = transform\n",
    "        self.unaligned = unaligned\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        A  = Image.open(self.files_A[index % len(self.files_A)]).convert('RGB')\n",
    "        if self.unaligned:\n",
    "            B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]).convert('RGB')\n",
    "        else:\n",
    "            B = Image.open(self.files_B[index % len(self.files_B)]).convert('RGB')\n",
    "            \n",
    "        # preprocessing\n",
    "        if self.transform is not None:\n",
    "            A = self.transform(A)\n",
    "            B = self.transform(B)\n",
    "        \n",
    "        return A, B\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34iB1seE1DAe"
   },
   "outputs": [],
   "source": [
    "# Select dataset\n",
    "data_folder = './data/'+use_dataset+'/'\n",
    "\n",
    "train_ds = DatasetFromFolder(root = data_folder, mode='train',\n",
    "                             transform = transforms.Compose([\n",
    "                                 transforms.Resize(int(img_size*1.12), Image.BICUBIC),\n",
    "                                 transforms.RandomCrop(img_size),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                             ])\n",
    "                            )\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymwTu5bl1DAl"
   },
   "source": [
    "## Training\n",
    "\n",
    "Before training the model we need a fuction, that stores a buffer of fakes, see [Shrivastava et al.](https://arxiv.org/abs/1612.07828)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rt8Y0M1ulJFJ"
   },
   "outputs": [],
   "source": [
    "class DHistBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def get_data(self, batch):\n",
    "        b_size,_,_,_ = batch.shape\n",
    "        to_return = []\n",
    "        \n",
    "        # The paper is not really clear what happens for batch size = 1, so I'll take code from aitorzip\n",
    "        # and randomly pick either from current or from history\n",
    "        if b_size==1:\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(batch)\n",
    "                to_return.append(batch)\n",
    "            else:\n",
    "                if random.rand() > 0.5:\n",
    "                    i = random.randint(0, self.max_size)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = batch\n",
    "                else:\n",
    "                    to_return.append(batch)\n",
    "        # The papers says that it takes half data from old buffered fakes and half from new generated\n",
    "        else:\n",
    "            # At the beginning we need to feed some data to the buffer\n",
    "            if len(self.data) == 0:\n",
    "                for b_element in batch:\n",
    "                    self.data.append(b_element.unsqueeze(0))\n",
    "                    to_return.append(b_element.unsqueeze(0))\n",
    "            else:\n",
    "                take_id_batch  = sample(range(b_size), int(b_size/2))\n",
    "                leave_id_batch = [x for x in range(b_size) if x not in take_id_batch]\n",
    "                random.shuffle(leave_id_batch)\n",
    "                take_id_hist   = sample(range(len(self.data)), int(b_size/2))\n",
    "                leave_id_hist = [x for x in range(b_size) if x not in take_id_batch]\n",
    "                random.shuffle(leave_id_hist)\n",
    "                \n",
    "                for b_element in batch[take_id_batch]:\n",
    "                    to_return.append(b_element.unsqueeze(0))\n",
    "                    \n",
    "                for idx in range(int(b_size/2)):\n",
    "                    to_return.append(self.data[take_id_hist[idx]].clone())\n",
    "                    self.data[take_id_hist[idx]] = batch[leave_id_batch[idx]].unsqueeze(0)\n",
    "                # Also the paper is not really clear for odd batch sizes.\n",
    "                # We again randomly pick from buffer or new generated\n",
    "                if b_size%2!=0:\n",
    "                    if random.rand() > 0.5:\n",
    "                        to_return.append(self.data[leave_id_hist[0]].clone())\n",
    "                        self.data[leave_id_hist[0]] = batch[leave_id_batch[-1]].unsqueeze(0)\n",
    "                    else:\n",
    "                        to_return.append(batch[leave_id_batch[-1]].unsqueeze(0))\n",
    "                \n",
    "                for b_element in batch:\n",
    "                    if len(self.data) < self.max_size:\n",
    "                        self.data.append(b_element.unsqueeze(0))\n",
    "        \n",
    "        # Shuffle so that new Fake images are not allways at the beginning of the batch\n",
    "        random.shuffle(to_return)\n",
    "        \n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSyBe0SDlJFV"
   },
   "source": [
    "### Compact function to get optimizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpiqvYdSlJFZ"
   },
   "outputs": [],
   "source": [
    "def get_cycleGAN(in_nc, out_nc, ngf, ndf, device, use_dataset, img_size, lr = 0.0002, beta1 = 0.5):\n",
    "    \n",
    "    # if img size = 128 use 6 res_blocks else 9\n",
    "    if img_size<=128:\n",
    "        n_res_blocks = 6\n",
    "    else:\n",
    "        n_res_blocks = 9\n",
    "    \n",
    "    # Get models\n",
    "    model_G_A2B = G_net(in_nc, out_nc, ngf, n_res_blocks).to(device)\n",
    "    model_G_B2A = G_net(out_nc, in_nc, ngf, n_res_blocks).to(device)\n",
    "    # Combine\n",
    "    Generators = (model_G_A2B, model_G_B2A)\n",
    "    \n",
    "    model_D_A = D_patch(in_nc, ndf).to(device)\n",
    "    model_D_B = D_patch(out_nc, ndf).to(device)\n",
    "    # Combine\n",
    "    Discrimators = (model_D_A, model_D_B)\n",
    "    \n",
    "    # Init weights\n",
    "    [m.apply(weights_init) for m in (model_G_A2B, model_G_B2A, model_D_B, model_D_A)]\n",
    "    # Set starting epoch to 0 as default\n",
    "    epoch_start    = 0\n",
    "    # Discriminator Buffer\n",
    "    D_buffer = ([], [])\n",
    "    \n",
    "    if os.path.isfile('./saved_models/cycleGAN_'+use_dataset+'_'+str(img_size)+'_saved_model.tar'):\n",
    "        pretrained = \"Users_answer\"\n",
    "        while pretrained not in [\"y\",\"n\"]:\n",
    "            pretrained = input(\"Pretrained Model available, use it? [y/n]:\")\n",
    "        # If User says \"y\", load weights\n",
    "        if pretrained==\"y\":\n",
    "            saved_data = torch.load('./saved_models/cycleGAN_'+use_dataset+'_'+str(img_size)+'_saved_model.tar',\n",
    "                                    map_location=device)\n",
    "            model_G_A2B.load_state_dict(saved_data['G_A2B_state_dict'])\n",
    "            model_G_B2A.load_state_dict(saved_data['G_B2A_state_dict'])\n",
    "            model_D_A.load_state_dict(saved_data['D_A_state_dict'])\n",
    "            model_D_B.load_state_dict(saved_data['D_B_state_dict'])\n",
    "            epoch_start = saved_data['current_epoch']\n",
    "            D_buffer = saved_data['fake_buffer']\n",
    "                \n",
    "    # Define Adam optimizer\n",
    "    opt_G = optim.Adam(list(model_G_A2B.parameters()) + list(model_G_B2A.parameters()),\n",
    "                       lr=lr, betas=(beta1,0.999))\n",
    "    opt_D = optim.Adam(list(model_D_A.parameters()) + list(model_D_B.parameters()),\n",
    "                       lr=lr, betas=(beta1,0.999))\n",
    "    \n",
    "    return Generators, Discrimators, opt_G, opt_D, epoch_start, D_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x41TJ1CQlJFi"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5Ekj2DV1DAo"
   },
   "outputs": [],
   "source": [
    "def fit_cycleGAN(epochs, G_models, D_models, opt_G, opt_D, train_dl, device, dis_buffer,\n",
    "                 lambs = (10., 5.), epoch_start = 0, epoch_lr_decline = 100, show_iter=None):\n",
    "    \n",
    "    # Check if start epoch is greater than epoch_start and stop function if so\n",
    "    if epochs<epoch_start:\n",
    "        return\n",
    "    \n",
    "    # Define Losses\n",
    "    GAN_crit   = nn.MSELoss()\n",
    "    Cycle_crit = nn.L1Loss()\n",
    "    Iden_crit  = nn.L1Loss()\n",
    "    \n",
    "    # Extract Models and optimizers\n",
    "    model_G_A2B, model_G_B2A = G_models\n",
    "    model_D_A, model_D_B     = D_models\n",
    "    \n",
    "    # Extract lambda\n",
    "    cyc_lamb, iden_lamb = lambs\n",
    "    \n",
    "    # Extract buffers\n",
    "    out_fake_A_buffer = DHistBuffer(max_size=50)\n",
    "    out_fake_B_buffer = DHistBuffer(max_size=50)\n",
    "    \n",
    "    ## Load data\n",
    "    out_fake_A_buffer.data, out_fake_B_buffer.data = dis_buffer\n",
    "    \n",
    "    # Define Learning Rate Decay\n",
    "    if epochs>epoch_lr_decline:\n",
    "        lambda_opt = lambda epoch: 1.0 - max(0, (epoch - epoch_lr_decline) / (epochs - epoch_lr_decline))\n",
    "    else:\n",
    "        lambda_opt = lambda epoch: 1.0\n",
    "    LR_scheduler_G = optim.lr_scheduler.LambdaLR(opt_G, lr_lambda=lambda_opt)\n",
    "    LR_scheduler_D = optim.lr_scheduler.LambdaLR(opt_D, lr_lambda=lambda_opt)\n",
    "    \n",
    "    # If not otherwise defined: show_iter = one epoch\n",
    "    if show_iter is None:\n",
    "        show_iter=len(train_dl)\n",
    "    \n",
    "    # Take time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        # Start Training Loop\n",
    "        for i, (A, B) in enumerate(train_dl):\n",
    "            # push images to device\n",
    "            real_A, real_B = A.to(device), B.to(device)\n",
    "            \n",
    "            ### Discriminator Training ###\n",
    "            opt_D.zero_grad()\n",
    "            \n",
    "            # Real pictures\n",
    "            ## For A\n",
    "            out_real_A   = model_D_A(real_A)\n",
    "            lossD_real_A = GAN_crit(out_real_A, torch.ones(out_real_A.size(), device=device))\n",
    "            ## For B\n",
    "            out_real_B   = model_D_B(real_B)\n",
    "            lossD_real_B = GAN_crit(out_real_B, torch.ones(out_real_B.size(), device=device))\n",
    "            ## Combine\n",
    "            lossD_real = lossD_real_A + lossD_real_B\n",
    "            \n",
    "            # Fake pictures\n",
    "            ## For A\n",
    "            out_fake_D_A = model_D_A(out_fake_A_buffer.get_data(model_G_B2A(real_B).detach()))\n",
    "            ### Loss function with all zeros for being fake\n",
    "            lossD_fake_A = GAN_crit(out_fake_D_A, torch.zeros(out_fake_D_A.size(), device=device))\n",
    "            ## For B\n",
    "            out_fake_D_B = model_D_B(out_fake_B_buffer.get_data(model_G_A2B(real_A).detach()))\n",
    "            ### Loss function with all zeros for being fake\n",
    "            lossD_fake_B = GAN_crit(out_fake_D_B, torch.zeros(out_fake_D_B.size(), device=device))\n",
    "            ## Combine\n",
    "            lossD_fake = lossD_fake_A + lossD_fake_B\n",
    "            \n",
    "            lossD = (lossD_real + lossD_fake)/2\n",
    "            \n",
    "            # Backprop\n",
    "            lossD.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            ### Generator Training ###\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            # GAN Loss\n",
    "            ## A->B\n",
    "            fake_B = model_G_A2B(real_A)\n",
    "            out_fake_B = model_D_B(fake_B)\n",
    "            GAN_loss_G_A2B = GAN_crit(out_fake_B, torch.ones(out_fake_B.size(), device=device))\n",
    "            ## B->A\n",
    "            fake_A = model_G_B2A(real_B)\n",
    "            out_fake_A = model_D_A(fake_A)\n",
    "            GAN_loss_G_B2A = GAN_crit(out_fake_A, torch.ones(out_fake_A.size(), device=device))\n",
    "            ## Combine\n",
    "            GAN_loss_G = GAN_loss_G_A2B + GAN_loss_G_B2A\n",
    "            \n",
    "            # Cycle Loss\n",
    "            ## A->B->A\n",
    "            recov_A  = model_G_B2A(fake_B)\n",
    "            Cycle_loss_A = Cycle_crit(recov_A, real_A)\n",
    "            ## B->A->B\n",
    "            recov_B  = model_G_A2B(fake_A)\n",
    "            Cycle_loss_B = Cycle_crit(recov_B, real_B)\n",
    "            ## Combine\n",
    "            Cycle_loss = Cycle_loss_A + Cycle_loss_B\n",
    "            \n",
    "            # Idendity Loss\n",
    "            Iden_loss_B = Iden_crit(model_G_A2B(real_B), real_B)\n",
    "            Iden_loss_A = Iden_crit(model_G_B2A(real_A), real_A)\n",
    "            ## Combine\n",
    "            Iden_loss = Iden_loss_B + Iden_loss_A\n",
    "            \n",
    "            # Total Loss\n",
    "            lossG = GAN_loss_G + cyc_lamb*Cycle_loss + iden_lamb*Iden_loss\n",
    "            \n",
    "            # Backprop\n",
    "            lossG.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            # Show some Optimazation metrics\n",
    "            if (i+epoch*len(train_dl))%show_iter == 0:\n",
    "                # get time values\n",
    "                hours, rem = divmod(time.time()-start_time, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                print('({:0>2}:{:0>2}:{:0>2}) [{}/{}][{}/{}] -> {:.2f}%\\tLoss_D: {:.4f}, D(x): {:.4f}\\tLoss_G: {:.4f}, D(G(z)): {:.4f}'.format(\n",
    "                    int(hours),int(minutes), int(seconds), epoch, epochs, i, len(train_dl),\n",
    "                    100*(i+epoch*len(train_dl))/(epochs*len(train_dl)),\n",
    "                    lossD, (out_real_A.mean()+out_real_B.mean())/2,\n",
    "                    lossG, (out_fake_D_A.mean()+out_fake_D_B.mean())/2))\n",
    "                \n",
    "            # Show currently inputs and Generated \n",
    "            if (i+epoch*len(train_dl))%(show_iter*5) == 0:\n",
    "                in_As    = real_A\n",
    "                fakes_B  = model_G_A2B(in_As).detach()\n",
    "                recovs_A = model_G_B2A(fakes_B).detach()\n",
    "                in_Bs    = real_B\n",
    "                fakes_A  = model_G_B2A(in_Bs).detach()\n",
    "                recovs_B = model_G_A2B(fakes_A).detach()\n",
    "                img_tmp = torch.cat([in_As, fakes_B, recovs_A, in_Bs, fakes_A, recovs_B], dim=0).cpu()\n",
    "                plt.figure(figsize=(8,8))\n",
    "                plt.axis(\"off\")\n",
    "                plt.imshow(np.transpose(vutils.make_grid(img_tmp, nrow=3, padding=1, normalize=True),(1,2,0)))\n",
    "                plt.pause(0.001)\n",
    "        \n",
    "        #############\n",
    "        # Save current state and epoch\n",
    "        torch.save({'G_A2B_state_dict': model_G_A2B.state_dict(),\n",
    "                    'G_B2A_state_dict': model_G_B2A.state_dict(),\n",
    "                    'D_A_state_dict': model_D_A.state_dict(),\n",
    "                    'D_B_state_dict': model_D_B.state_dict(),\n",
    "                    'current_epoch': epoch+1,\n",
    "                    'fake_buffer': (out_fake_A_buffer.data, out_fake_B_buffer.data),\n",
    "                   },'./saved_models/cycleGAN_'+use_dataset+'_'+str(img_size)+'_saved_model.tar')\n",
    "        \n",
    "        # Making a step for the LR scheduler\n",
    "        LR_scheduler_G.step()\n",
    "        LR_scheduler_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5eEIWYAI1DAu"
   },
   "outputs": [],
   "source": [
    "# Get initialized models and optimizers\n",
    "G_models, D_models, opt_G, opt_D, epoch_start, buffers = get_cycleGAN(in_nc=3, out_nc=3,\n",
    "                                                                      ngf=64, ndf=64,\n",
    "                                                                      device=device,\n",
    "                                                                      use_dataset=use_dataset,\n",
    "                                                                      img_size=img_size,\n",
    "                                                                      lr=lr, beta1=beta1)\n",
    "\n",
    "# Fit CycleGAN\n",
    "fit_cycleGAN(epochs, G_models, D_models, opt_G, opt_D, train_dl, device, buffers,\n",
    "             lambs=lambs, epoch_start=epoch_start, epoch_lr_decline=epoch_lr_decline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCz7dJ401DA5"
   },
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA22ohwH1DA9"
   },
   "outputs": [],
   "source": [
    "# Get Test data\n",
    "test_ds = DatasetFromFolder(root = data_folder, mode='test',\n",
    "                             transform = transforms.Compose([\n",
    "                                 transforms.Resize(int(img_size*1.12), Image.BICUBIC),\n",
    "                                 transforms.RandomCrop(img_size),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                             ])\n",
    "                            )\n",
    "\n",
    "test_dl = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "# Get generating Models\n",
    "model_G_A2B, model_G_B2A = G_models\n",
    "# Load data\n",
    "saved_data = torch.load('./saved_models/cycleGAN_'+use_dataset+'_'+str(img_size)+'_saved_model.tar',\n",
    "                        map_location=device)\n",
    "model_G_A2B.load_state_dict(saved_data['G_A2B_state_dict'])\n",
    "model_G_B2A.load_state_dict(saved_data['G_B2A_state_dict'])\n",
    "\n",
    "# Push models to cpu\n",
    "model_G_A2B = model_G_A2B.cpu()\n",
    "model_G_B2A = model_G_B2A.cpu()\n",
    "\n",
    "test_img = next(iter(test_dl))\n",
    "# For A\n",
    "A_test   = test_img[0].cpu()\n",
    "fakes_B  = model_G_A2B(A_test).detach()\n",
    "recovs_A = model_G_B2A(fakes_B).detach()\n",
    "# For B\n",
    "B_test   = test_img[1].cpu()\n",
    "fakes_A  = model_G_B2A(B_test).detach()\n",
    "recovs_B = model_G_A2B(fakes_A).detach()\n",
    "\n",
    "img_tmp = torch.cat([A_test, fakes_B, recovs_A, B_test, fakes_A, recovs_B], dim=0).cpu()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(img_tmp, nrow=3, padding=1, normalize=True),(1,2,0)))\n",
    "plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTjLV_mZlJGE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cycleGAN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (Pap2PyT)",
   "language": "python",
   "name": "pap2pyt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
