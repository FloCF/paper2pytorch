{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJzhKAiA1C-4"
   },
   "source": [
    "# [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)\n",
    "\n",
    "### Jun-Yan Zhu, Taesung Park, Phillip Isola & Alexei A. Efros\n",
    "\n",
    "Algorithm for unpaired image-to-image translation. Below are some applications and results:\n",
    "\n",
    "<img src=\"images/cycleGAN-teaser.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the cycleGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2537,
     "status": "ok",
     "timestamp": 1554206920497,
     "user": {
      "displayName": "Florian C. F. Schulz",
      "photoUrl": "",
      "userId": "08634931705470539405"
     },
     "user_tz": -120
    },
    "id": "jTnRx5NK1C-_",
    "outputId": "0f01c6af-0806-47bf-dcc7-a9d730a0334d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Define by User ###\n",
    "# Choose dataset, available [\"ae_photos\",\"apple2orange\", \"cezanne2photo\", \"cityscapes\", \"facades\",\n",
    "#                            \"horse2zebra\", \"iphone2dslr_flower\", \"maps\", \"mini\", \"mini_colorization\",\n",
    "#                            \"mini_pix2pix\", \"monet2photo\", \"summer2winter_yosemite\", \"ukiyoe2photo\", \"vangogh2photo\"]\n",
    "use_dataset = 'horse2zebra'\n",
    "# Define Image size\n",
    "img_size = 256\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloab Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "mkdir: cannot create directory ‘saved_models’: File exists\n",
      "WARNING: timestamping does nothing in combination with -O. See the manual\n",
      "for details.\n",
      "\n",
      "--2019-06-24 10:40:04--  https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
      "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.189.73\n",
      "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.189.73|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 116867962 (111M) [application/zip]\n",
      "Saving to: ‘data/data.zip’\n",
      "\n",
      "data/data.zip       100%[===================>] 111.45M  5.71MB/s    in 24s     \n",
      "\n",
      "2019-06-24 10:40:30 (4.56 MB/s) - ‘data/data.zip’ saved [116867962/116867962]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data saved_models\n",
    "# Change dataset in URL use same name as und use_dataset above\n",
    "!wget -N \"https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\" -O data/data.zip\n",
    "!unzip -qq -o data/data.zip -d data/\n",
    "!rm data/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yudMFKeU1C_f"
   },
   "source": [
    "## Define the two Generator Networks\n",
    "\n",
    "Following the archetecture of transformation net form [Johnson et al.](https://cs.stanford.edu/people/jcjohns/eccv16/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-wFaxcTF1C_l"
   },
   "outputs": [],
   "source": [
    "class Basic_Layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, norm_layer, use_relu=True):\n",
    "        super(Basic_Layer, self).__init__()\n",
    "        self.use_relu = use_relu\n",
    "        self.pad  = nn.ReflectionPad2d(kernel_size // 2)\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride)\n",
    "        self.norm = norm_layer(out_ch)\n",
    "       \n",
    "    def forward(self, input):\n",
    "        input = self.pad(input)\n",
    "        if self.use_relu:\n",
    "            out = F.relu(self.norm(self.conv(input)), inplace=True)\n",
    "        else:\n",
    "            out = self.norm(self.conv(input))\n",
    "        return out\n",
    "        \n",
    "class Res_Block(nn.Module):\n",
    "    def __init__(self, n_ch, norm_layer):\n",
    "        super(Res_Block, self).__init__()\n",
    "        self.layer1 = Basic_Layer(n_ch, n_ch, kernel_size=3, stride=1, norm_layer=norm_layer)\n",
    "        self.layer2 = Basic_Layer(n_ch, n_ch, kernel_size=3, stride=1, norm_layer=norm_layer,\n",
    "                                  use_relu=False)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        identity = input \n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input) \n",
    "        out   = input + identity\n",
    "        return out\n",
    "\n",
    "    \n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, norm_layer):\n",
    "        super(Upsample, self).__init__()\n",
    "        self.deconv = nn.ConvTranspose2d(in_ch, out_ch, kernel_size, 2, padding=kernel_size // 2, output_padding=1)\n",
    "        self.norm   = norm_layer(out_ch)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        out = F.relu(self.norm(self.deconv(input)), inplace=True)\n",
    "        return out\n",
    "\n",
    "class G_net(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, ngf=64, n_res_blocks=6, norm_layer=nn.InstanceNorm2d):\n",
    "        super(G_net, self).__init__()\n",
    "        # Define Encoding layers\n",
    "        self.enco1  = Basic_Layer(in_nc, ngf, kernel_size=7, stride=1, norm_layer=norm_layer)\n",
    "        self.enco2  = Basic_Layer(ngf, ngf*2, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        self.enco3  = Basic_Layer(ngf*2, ngf*4, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        # Define Residual layers\n",
    "        self.residual = nn.Sequential(*[Res_Block(ngf*4, norm_layer=norm_layer)]*n_res_blocks)\n",
    "        # Define Decoding layers\n",
    "        self.deco1  = Upsample(ngf*4, ngf*2, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        self.deco2  = Upsample(ngf*2, ngf, kernel_size=3, stride=2, norm_layer=norm_layer)\n",
    "        self.deco3  = nn.Conv2d(ngf, out_nc, kernel_size=7, stride=1, padding=3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Encoding\n",
    "        input = self.enco1(input)\n",
    "        input = self.enco2(input)\n",
    "        input = self.enco3(input)\n",
    "        # Residual\n",
    "        input = self.residual(input)\n",
    "        # Decoding\n",
    "        input = self.deco1(input)\n",
    "        input = self.deco2(input)\n",
    "        input = self.deco3(input)\n",
    "        return torch.tanh(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmQEos_e1C_t"
   },
   "source": [
    "### Define Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFUXLDI51C_v"
   },
   "outputs": [],
   "source": [
    "# Define 70x70 PatchGAN Discriminator\n",
    "class D_patch(nn.Module):\n",
    "    def __init__(self, in_nc, ndf=64, norm_layer = nn.InstanceNorm2d):\n",
    "        super(D_patch, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_nc, ndf, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm2 = norm_layer(ndf*2)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm3 = norm_layer(ndf*4)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm4 = norm_layer(ndf*8)\n",
    "        self.final = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = F.leaky_relu(self.conv1(input), negative_slope=0.2, inplace=True)\n",
    "        input = F.leaky_relu(self.norm2(self.conv2(input)), negative_slope=0.2, inplace=True)\n",
    "        input = F.leaky_relu(self.norm3(self.conv3(input)), negative_slope=0.2, inplace=True)\n",
    "        input = F.leaky_relu(self.norm4(self.conv4(input)), negative_slope=0.2, inplace=True)\n",
    "        return torch.sigmoid(self.final(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wlm8PKM51C_1"
   },
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qT48CEma1C_3"
   },
   "outputs": [],
   "source": [
    "# custom weights initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight, mean=1.0, std=0.02)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHSsTMJ51C__"
   },
   "source": [
    "#### Compact function to get optimizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-T8bEBn1DAF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_cycleGAN(in_nc, out_nc, ngf, ndf, device, use_dataset, img_size, lr = 0.0002, beta1 = 0.5):\n",
    "    \n",
    "    # if img size = 128 use 6 res_blocks else 9\n",
    "    if img_size<=128:\n",
    "        n_res_blocks = 6\n",
    "    else:\n",
    "        n_res_blocks = 9\n",
    "    \n",
    "    # Get models\n",
    "    model_G_A2B = G_net(in_nc, out_nc, ngf, n_res_blocks).to(device)\n",
    "    model_G_B2A = G_net(out_nc, in_nc, ngf, n_res_blocks).to(device)\n",
    "    # Combine\n",
    "    Generators = (model_G_A2B, model_G_B2A)\n",
    "    \n",
    "    model_D_A = D_patch(in_nc, ndf).to(device)\n",
    "    model_D_B = D_patch(out_nc, ndf).to(device)\n",
    "    # Combine\n",
    "    Discrimators = (model_D_A, model_D_B)\n",
    "    \n",
    "    # Init weights\n",
    "    [m.apply(weights_init) for m in (model_G_A2B, model_G_B2A, model_D_B, model_D_A)]\n",
    "    # Set starting epoch to 0 as default\n",
    "    epoch_start    = 0\n",
    "    # Init saved losses\n",
    "    saved_losses_G = {'A2B':[], 'B2A':[]}\n",
    "    saved_losses_D = {'A':[], 'B':[]}\n",
    "    \n",
    "    if os.path.isfile('./saved_models/cycleGAN_'+use_dataset+'_saved_model.tar'):\n",
    "        pretrained = \"Users_answer\"\n",
    "        while pretrained not in [\"y\",\"n\"]:\n",
    "            pretrained = input(\"Pretrained Model available, use it? [y/n]:\")\n",
    "        # If User says \"y\", load weights\n",
    "        if pretrained==\"y\":\n",
    "            saved_data = torch.load('./saved_models/cycleGAN_'+use_dataset+'_saved_model.tar',\n",
    "                                    map_location=device)\n",
    "            model_G_A2B.load_state_dict(saved_data['G_A2B_state_dict'])\n",
    "            model_G_B2A.load_state_dict(saved_data['G_B2A_state_dict'])\n",
    "            model_D_A.load_state_dict(saved_data['D_A_state_dict'])\n",
    "            model_D_B.load_state_dict(saved_data['D_B_state_dict'])\n",
    "            epoch_start = saved_data['current_epoch']\n",
    "            saved_losses_G = saved_data['losses_G']\n",
    "            saved_losses_D = saved_data['losses_D']\n",
    "            \n",
    "    # Define Adam optimizer\n",
    "    opt_G = optim.Adam(list(model_G_A2B.parameters()) + list(model_G_B2A.parameters()),\n",
    "                       lr=lr, betas=(beta1,0.999))\n",
    "    opt_D = optim.Adam(list(model_D_A.parameters()) + list(model_D_B.parameters()),\n",
    "                       lr=lr, betas=(beta1,0.999))\n",
    "    \n",
    "    return Generators, Discrimators, opt_G, opt_D, epoch_start, (saved_losses_G, saved_losses_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HEgWqjaI1DAN"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZalYGkd1DAR"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "class DatasetFromFolder(Dataset):\n",
    "    def __init__(self, root, mode='train', unaligned=True, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to folders for train, val amd test\n",
    "            mode (str): Either 'train' or 'test'\n",
    "            direction (str): Either 'AtoB' or 'BtoA' indicating which direction the prediction should go\n",
    "            unaligned (bool): If unpaired or paired dataset\n",
    "            transform (torchvision obj) : Usual image preprocessing\n",
    "        \"\"\"\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.files_A   = [os.path.join(root, '%sA/' % mode) + x for x in sorted(os.listdir(os.path.join(root, '%sA/' % mode)))]\n",
    "        self.files_B   = [os.path.join(root, '%sB/' % mode) + x for x in sorted(os.listdir(os.path.join(root, '%sB/' % mode)))]\n",
    "        self.transform = transform\n",
    "        self.unaligned = unaligned\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        A  = Image.open(self.files_A[index % len(self.files_A)]).convert('RGB')\n",
    "        if self.unaligned:\n",
    "            B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]).convert('RGB')\n",
    "        else:\n",
    "            B = Image.open(self.files_B[index % len(self.files_B)]).convert('RGB')\n",
    "            \n",
    "        # preprocessing\n",
    "        if self.transform is not None:\n",
    "            A = self.transform(A)\n",
    "            B = self.transform(B)\n",
    "        \n",
    "        return A, B\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "34iB1seE1DAe"
   },
   "outputs": [],
   "source": [
    "# Select dataset\n",
    "data_folder = './data/'+use_dataset+'/'\n",
    "\n",
    "train_ds = DatasetFromFolder(root = data_folder, mode='train',\n",
    "                             transform = transforms.Compose([\n",
    "                                 transforms.Resize(int(img_size*1.12), Image.BICUBIC),\n",
    "                                 transforms.RandomCrop(img_size),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                             ])\n",
    "                            )\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymwTu5bl1DAl"
   },
   "source": [
    "## Train Model\n",
    "\n",
    "Before training the model we need a fuction, that stores a buffer of fakes, see [Shrivastava et al.](https://arxiv.org/abs/1612.07828)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "class DHistBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def get_data(self, batch):\n",
    "        b_size,_,_,_ = batch.shape\n",
    "        to_return = []\n",
    "        \n",
    "        # The paper is not really clear what happens for batch size = 1, so I'll take code from aitorzip\n",
    "        # and randomly pick either from current or from history\n",
    "        if b_size==1:\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(batch)\n",
    "                to_return.append(batch)\n",
    "            else:\n",
    "                if random.rand() > 0.5:\n",
    "                    i = random.randint(0, self.max_size)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = batch\n",
    "                else:\n",
    "                    to_return.append(b_element)\n",
    "        else:\n",
    "            if len(self.data) == 0:\n",
    "                for b_element in batch:\n",
    "                    self.data.append(b_element.unsqueeze(0))\n",
    "                    to_return.append(b_element.unsqueeze(0))\n",
    "            else:\n",
    "                take_id_batch  = sample(range(b_size), int(b_size/2))\n",
    "                leave_id_batch = [x for x in range(b_size) if x not in take_id_batch]\n",
    "                random.shuffle(leave_id_batch)\n",
    "                take_id_hist   = sample(range(len(self.data)), int(b_size/2))\n",
    "                leave_id_hist = [x for x in range(b_size) if x not in take_id_batch]\n",
    "                random.shuffle(leave_id_hist)\n",
    "                \n",
    "                for b_element in batch[take_id_batch]:\n",
    "                    to_return.append(b_element.unsqueeze(0))\n",
    "                    \n",
    "                for idx in range(int(b_size/2)):\n",
    "                    to_return.append(self.data[take_id_hist[idx]].clone())\n",
    "                    self.data[take_id_hist[idx]] = batch[leave_id_batch[idx]]\n",
    "                # Also not really clear for odd batch size\n",
    "                if b_size%2!=0:\n",
    "                    if random.rand() > 0.5:\n",
    "                        to_return.append(self.data[leave_id_hist[0]].clone())\n",
    "                        self.data[leave_id_hist[0]] = batch[leave_id_batch[-1]]\n",
    "                    else:\n",
    "                        to_return.append(b_element)\n",
    "        print([out.shape for out in to_return])\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = DHistBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 3, 128, 128]), torch.Size([1, 3, 128, 128]), torch.Size([1, 3, 128, 128]), torch.Size([1, 3, 128, 128])]\n",
      "torch.Size([4, 3, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = test.get_data(torch.randn(4,3,128,128))\n",
    "print(out.shape)\n",
    "len(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W5Ekj2DV1DAo"
   },
   "outputs": [],
   "source": [
    "def fit_cycleGAN(epochs, G_models, D_models, opt_G, opt_D, train_dl, device, lambs = (10., 5.),\n",
    "                 epoch_start = 0, epoch_lr_decline = 100, show_iter=50):\n",
    "    \n",
    "    # Check if start epoch is greater than epoch_start and stop function if so\n",
    "    if epochs<epoch_start:\n",
    "        return\n",
    "    \n",
    "    # Define Losses\n",
    "    GAN_crit   = nn.MSELoss()\n",
    "    Cycle_crit = nn.L1Loss()\n",
    "    Iden_crit  = nn.L1Loss()\n",
    "    \n",
    "    # Extract Models and optimizers\n",
    "    model_G_A2B, model_G_B2A = G_models\n",
    "    model_D_A, model_D_B     = D_models\n",
    "    \n",
    "    # Extract lambda\n",
    "    cyc_lamb, iden_lamb = lambs\n",
    "    \n",
    "    # Define Learning Rate Decay\n",
    "    if epochs>epoch_lr_decline:\n",
    "        lambda_opt = lambda epoch: 1.0 - max(0, (epoch - epoch_lr_decline) / (epochs - epoch_lr_decline))\n",
    "    else:\n",
    "        lambda_opt = lambda epoch: 1.0\n",
    "    LR_scheduler_G = optim.lr_scheduler.LambdaLR(opt_G, lr_lambda=lambda_opt)\n",
    "    LR_scheduler_D = optim.lr_scheduler.LambdaLR(opt_D, lr_lambda=lambda_opt)\n",
    "    \n",
    "    # Take time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        LR_scheduler_G.step()\n",
    "        LR_scheduler_D.step()\n",
    "        # Start Training Loop\n",
    "        for i, (A, B) in enumerate(train_dl):\n",
    "            # push images to device\n",
    "            real_A, real_B = A.to(device), B.to(device)\n",
    "            \n",
    "            ### Discriminator Training ###\n",
    "            opt_D.zero_grad()\n",
    "            \n",
    "            # Real pictures\n",
    "            ## For A\n",
    "            out_real_A   = model_D_A(real_A)\n",
    "            lossD_real_A = GAN_crit(out_real_A, torch.ones(out_real_A.size(), device=device))\n",
    "            ## For B\n",
    "            out_real_B   = model_D_B(real_B)\n",
    "            lossD_real_B = GAN_crit(out_real_B, torch.ones(out_real_B.size(), device=device))\n",
    "            ## Combine\n",
    "            lossD_real = lossD_real_A + lossD_real_B\n",
    "            \n",
    "            # Fake pictures\n",
    "            ## For A\n",
    "            out_fake_D_A = model_D_A(model_G_B2A(real_B).detach())\n",
    "            ### Loss function with all zeros for being fake\n",
    "            lossD_fake_A = GAN_crit(out_fake_D_A, torch.zeros(out_fake_D_A.size(), device=device))\n",
    "            ## For B\n",
    "            out_fake_D_B = model_D_B(model_G_A2B(real_A).detach())\n",
    "            ### Loss function with all zeros for being fake\n",
    "            lossD_fake_B = GAN_crit(out_fake_D_B, torch.zeros(out_fake_D_B.size(), device=device))\n",
    "            ## Combine\n",
    "            lossD_fake = lossD_fake_A + lossD_fake_B\n",
    "            \n",
    "            lossD = (lossD_real + lossD_fake)/2\n",
    "            \n",
    "            # Backprop\n",
    "            lossD.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            ### Generator Training ###\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            # GAN Loss\n",
    "            ## A->B\n",
    "            fake_B = model_G_A2B(real_A)\n",
    "            out_fake_B = model_D_B(fake_B)\n",
    "            GAN_loss_G_A2B = GAN_crit(out_fake_B, torch.ones(out_fake_B.size(), device=device))\n",
    "            ## B->A\n",
    "            fake_A = model_G_B2A(real_B)\n",
    "            out_fake_A = model_D_A(fake_A)\n",
    "            GAN_loss_G_B2A = GAN_crit(out_fake_A, torch.ones(out_fake_A.size(), device=device))\n",
    "            ## Combine\n",
    "            GAN_loss_G = GAN_loss_G_A2B + GAN_loss_G_B2A\n",
    "            \n",
    "            # Cycle Loss\n",
    "            ## A->B->A\n",
    "            recov_A  = model_G_B2A(fake_B)\n",
    "            Cycle_loss_A = Cycle_crit(recov_A, real_A)\n",
    "            ## B->A->B\n",
    "            recov_B  = model_G_A2B(fake_A)\n",
    "            Cycle_loss_B = Cycle_crit(recov_B, real_B)\n",
    "            ## Combine\n",
    "            Cycle_loss = Cycle_loss_A + Cycle_loss_B\n",
    "            \n",
    "            # Idendity Loss\n",
    "            Iden_loss_B = Iden_crit(model_G_A2B(real_B), real_B)\n",
    "            Iden_loss_A = Iden_crit(model_G_B2A(real_A), real_A)\n",
    "            ## Combine\n",
    "            Iden_loss = Iden_loss_B + Iden_loss_A\n",
    "            \n",
    "            # Total Loss\n",
    "            lossG = GAN_loss_G + cyc_lamb*Cycle_loss + iden_lamb*Iden_loss\n",
    "            \n",
    "            # Backprop\n",
    "            lossG.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            # Show some Optimazation metrics\n",
    "            if (i+epoch*len(train_dl))%show_iter == 0:\n",
    "                # get time values\n",
    "                hours, rem = divmod(time.time()-start_time, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                print('({:0>2}:{:0>2}:{:0>2}) [{}/{}][{}/{}] -> {:.2f}%\\tLoss_D: {:.4f}, D(x): {:.4f}\\tLoss_G: {:.4f}, D(G(z)): {:.4f}'.format(\n",
    "                    int(hours),int(minutes), int(seconds), epoch, epochs, i, len(train_dl),\n",
    "                    100*(i+epoch*len(train_dl))/(epochs*len(train_dl)),\n",
    "                    lossD, (out_real_A.mean()+out_real_B.mean())/2,\n",
    "                    lossG, (out_fake_D_A.mean()+out_fake_D_B.mean())/2))\n",
    "                \n",
    "            # Show currently inputs and Generated \n",
    "            if (i+epoch*len(train_dl))%1334 == 0:\n",
    "                in_As    = real_A\n",
    "                fakes_B  = model_G_A2B(in_As).detach()\n",
    "                recovs_A = model_G_B2A(fakes_B).detach()\n",
    "                in_Bs    = real_B\n",
    "                fakes_A  = model_G_B2A(in_Bs).detach()\n",
    "                recovs_B = model_G_A2B(fakes_A).detach()\n",
    "                img_tmp = torch.cat([in_As, fakes_B, recovs_A, in_Bs, fakes_A, recovs_B], dim=0, ).cpu()\n",
    "                plt.figure(figsize=(12,12))\n",
    "                plt.axis(\"off\")\n",
    "                plt.imshow(np.transpose(vutils.make_grid(img_tmp, nrow=3, padding=2, normalize=True),(1,2,0)))\n",
    "                plt.pause(0.001)\n",
    "        \n",
    "        #############\n",
    "        # Save current state and epoch\n",
    "        torch.save({'G_A2B_state_dict': model_G_A2B.state_dict(),\n",
    "                    'G_B2A_state_dict': model_G_B2A.state_dict(),\n",
    "                    'D_A_state_dict': model_D_A.state_dict(),\n",
    "                    'D_B_state_dict': model_D_B.state_dict(),\n",
    "                    'current_epoch': epoch+1,\n",
    "                   },'./saved_models/cycleGAN_'+use_dataset+'_saved_model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 24353,
     "output_embedded_package_id": "1yi0qVNh-_urFYe6WARsIpOgCacahi3An"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4341513,
     "status": "error",
     "timestamp": 1554218516728,
     "user": {
      "displayName": "Florian C. F. Schulz",
      "photoUrl": "",
      "userId": "08634931705470539405"
     },
     "user_tz": -120
    },
    "id": "5eEIWYAI1DAu",
    "outputId": "59b44cff-f1ac-4924-85db-dd1e7bab2be3"
   },
   "outputs": [],
   "source": [
    "# Get initialized models and optimizers\n",
    "G_models, D_models, opt_G, opt_D, epoch_start = get_cycleGAN(in_nc=3, out_nc=3, ngf=64, ndf=64, device=device, lr = 0.0002, beta1 = 0.5)\n",
    "# Fit CycleGAN\n",
    "fit_cycleGAN(200, G_models, D_models, opt_G, opt_D, train_dl, device, lambs = (10.,0.),\n",
    "             epoch_start = epoch_start, epoch_lr_decline = 100, show_iter=667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LCz7dJ401DA5"
   },
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA22ohwH1DA9"
   },
   "outputs": [],
   "source": [
    "# Get Test data\n",
    "test_ds = DatasetFromFolder(root = data_folder, mode='test',\n",
    "                             transform = transforms.Compose([\n",
    "                                 transforms.Resize(int(img_size*1.12), Image.BICUBIC),\n",
    "                                 transforms.RandomCrop(img_size),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "                             ])\n",
    "                            )\n",
    "\n",
    "test_dl = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "# Get generating Models\n",
    "model_G_A2B, model_G_B2A = G_models\n",
    "# Load data\n",
    "saved_data = torch.load('./saved_models/cycleGAN_'+use_dataset+'_saved_model.tar',\n",
    "                        map_location=device)\n",
    "model_G_A2B.load_state_dict(saved_data['G_A2B_state_dict'])\n",
    "model_G_B2A.load_state_dict(saved_data['G_B2A_state_dict'])\n",
    "            \n",
    "\n",
    "test_img = next(iter(test_dl))\n",
    "# For A\n",
    "A_test   = test_img[0].cpu()\n",
    "fakes_B  = model_G_A2B(A_test).detach()\n",
    "recovs_A = model_G_B2A(fakes_B).detach()\n",
    "# For B\n",
    "B_test   = test_img[1].cpu()\n",
    "fakes_A  = model_G_B2A(B_test).detach()\n",
    "recovs_B = model_G_A2B(fakes_A).detach()\n",
    "\n",
    "img_tmp = torch.cat([A_test, fakes_B, recovs_A, B_test, fakes_A, recovs_B], dim=0, ).cpu()\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(img_tmp, nrow=3, padding=2, normalize=True),(1,2,0)))\n",
    "plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cu2vqN9D1DBG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cycleGAN.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
