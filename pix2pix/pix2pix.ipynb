{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n",
    "\n",
    "### Phillip Isola, Jun-Yan Zhu, Tinghui Zhou & Alexei A. Efros\n",
    "\n",
    "Algorithm that allows any image-to-image translation. Below are some applications and results:\n",
    "\n",
    "<img src=\"images/pix2pix-teaser.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the pix2pix Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### Define by User ###\n",
    "# Choose dataset, available [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\"]\n",
    "use_dataset = 'cityscapes'\n",
    "direction='BtoA'\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goggle Colab Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data saved_models\n",
    "# Change dataset in URL use same name as und use_dataset above\n",
    "!wget -N \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/cityscapes.tar.gz\" -O data/data.tar.gz\n",
    "!tar -xzf data/data.tar.gz -C data/\n",
    "!rm data/data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_unet(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, ngf):\n",
    "        super(G_unet, self).__init__()\n",
    "        ## ENCODER ##\n",
    "        # 1st encoder layer input: batch_size x 3 x 256 x 256 (for in_nc = 3)\n",
    "        self.e1    = nn.Conv2d(in_nc, ngf, kernel_size=4, stride=2, padding=1)\n",
    "        # 2nd encoder layer input: batch_size x 64 x 128 x 128 (for ngf = 64)\n",
    "        self.e2    = nn.Conv2d(ngf, ngf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf*2)\n",
    "        # 3rd encoder layer input: batch_size x 128 x 64 x 64 (for ngf = 64)\n",
    "        self.e3    = nn.Conv2d(ngf*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf*4)\n",
    "        # 4th encoder layer input: batch_size x 256 x 32 x 32 (for ngf = 64)\n",
    "        self.e4    = nn.Conv2d(ngf*4, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf*8)\n",
    "        # 5th to 8th endocer layer input: batch_size x 512 x 16 x 16 (for ngf = 64)\n",
    "        self.e5  = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(ngf*8)\n",
    "        # 6th endocer layer input: batch_size x 512 x 8 x 8 (for ngf = 64)\n",
    "        self.e6  = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(ngf*8)\n",
    "        # 7th endocer layer input: batch_size x 512 x 4 x 4 (for ngf = 64)\n",
    "        self.e7  = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn7 = nn.BatchNorm2d(ngf*8)\n",
    "        # 8th endocer layer input: batch_size x 512 x 2 x 2 (for ngf = 64)\n",
    "        self.e8  = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        ## DECODER ##\n",
    "        # 1st decoder layer input: batch_size x 512 x 1 x 1 (for ngf = 64)\n",
    "        self.d1    = nn.ConvTranspose2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn9 = nn.BatchNorm2d(ngf*8)\n",
    "        # 2nd decoder layer input: batch_size x 512 x 2 x 2 (for ngf = 64)\n",
    "        self.d2  = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn10 = nn.BatchNorm2d(ngf*8)\n",
    "        # 3rd decoder layer input: batch_size x 512 x 4 x 4 (for ngf = 64)\n",
    "        self.d3  = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn11 = nn.BatchNorm2d(ngf*8)\n",
    "        # 4th decoder layer input: batch_size x 512 x 8 x 8 (for ngf = 64)\n",
    "        self.d4  = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn12 = nn.BatchNorm2d(ngf*8)\n",
    "        # 3rd to 5th decoder layer input: batch_size x 512 x 4->8->16 x 4->8->16 (for ngf = 64)\n",
    "        self.d5    = nn.ConvTranspose2d(ngf*8*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn13 = nn.BatchNorm2d(ngf*4)\n",
    "        # 6th decoder layer input: batch_size x 512 x 32 x 32 (for ngf = 64)\n",
    "        self.d6    = nn.ConvTranspose2d(ngf*4*2, ngf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn14 = nn.BatchNorm2d(ngf*2)\n",
    "        # 7th decoder layer input: batch_size x 512 x 64 x 64 (for ngf = 64)\n",
    "        self.d7    = nn.ConvTranspose2d(ngf*2*2, ngf, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.bn15 = nn.BatchNorm2d(ngf)\n",
    "        # 7th decoder layer input: batch_size x 512 x 128 x 128 (for ngf = 64)\n",
    "        self.d8    = nn.ConvTranspose2d(ngf*2, out_nc, kernel_size=4, stride=2, padding=1)\n",
    "        # Output size: batch_size x 3 x 256 x 256 (for out_nc = 3)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # 1st encoder-layer\n",
    "        u_e1 = F.leaky_relu(self.e1(input), negative_slope=0.2, inplace=True)\n",
    "        # 2nd encoder-layer\n",
    "        u_e2 = F.leaky_relu(self.bn2(self.e2(u_e1)), negative_slope=0.2, inplace=True)\n",
    "        # 3rd encoder-layer\n",
    "        u_e3 = F.leaky_relu(self.bn3(self.e3(u_e2)), negative_slope=0.2, inplace=True)\n",
    "        # 4th encoder-Layer\n",
    "        u_e4 = F.leaky_relu(self.bn4(self.e4(u_e3)), negative_slope=0.2, inplace=True)\n",
    "        # 5th encoder-layer\n",
    "        u_e5 = F.leaky_relu(self.bn5(self.e5(u_e4)), negative_slope=0.2, inplace=True)\n",
    "        # 6th encoder-layer\n",
    "        u_e6 = F.leaky_relu(self.bn6(self.e6(u_e5)), negative_slope=0.2, inplace=True)\n",
    "        # 7th encoder-layer\n",
    "        u_e7 = F.leaky_relu(self.bn7(self.e7(u_e6)), negative_slope=0.2, inplace=True)\n",
    "        # Bottleneck\n",
    "        u_e8 = F.relu(self.e8(u_e7), inplace=True)\n",
    "        # 1st decoder layer\n",
    "        u_d1 = F.dropout2d(self.bn9(self.d1(u_e8)), p=0.5)\n",
    "        u_d1 = F.relu(torch.cat([u_d1, u_e7], dim=1), inplace=True)\n",
    "        # 2nd decoder layer\n",
    "        u_d2 = F.dropout2d(self.bn10(self.d2(u_d1)), p=0.5)\n",
    "        u_d2 = F.relu(torch.cat([u_d2, u_e6], dim=1), inplace=True)\n",
    "        # 3rd decoder layer\n",
    "        u_d3 = F.dropout2d(self.bn11(self.d3(u_d2)), p=0.5)\n",
    "        u_d3 = F.relu(torch.cat([u_d3, u_e5], dim=1), inplace=True)\n",
    "        # 4th decoder layer\n",
    "        u_d4 = F.relu(torch.cat([self.bn12(self.d4(u_d3)), u_e4], dim=1), inplace=True)\n",
    "        # 5th decoder layer\n",
    "        u_d5 = F.relu(torch.cat([self.bn13(self.d5(u_d4)), u_e3], dim=1), inplace=True)\n",
    "        # 6th decoder layer\n",
    "        u_d6 = F.relu(torch.cat([self.bn14(self.d6(u_d5)), u_e2], dim=1), inplace=True)\n",
    "        # 7th decoder layer\n",
    "        u_d7 = F.relu(torch.cat([self.bn15(self.d7(u_d6)), u_e1], dim=1), inplace=True)\n",
    "        # Final layer\n",
    "        return torch.tanh(self.d8(u_d7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class D_basic_layer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=4, stride=2, padding=1):\n",
    "        super(D_basic_layer, self).__init__()\n",
    "        self.conv  = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=False)\n",
    "        self.bn    = nn.BatchNorm2d(out_ch)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return F.leaky_relu(self.bn(self.conv(input)), negative_slope=0.2, inplace=True)\n",
    "\n",
    "class D_basic(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, ndf, n_layer=3):\n",
    "        super(D_basic, self).__init__()\n",
    "        # Define PixelGAN if n_layers=0\n",
    "        if n_layer==0:\n",
    "            self.conv1 = nn.Conv2d(in_nc+out_nc, ndf, kernel_size=1, stride=1, padding=0)\n",
    "            self.main  = nn.Sequential(OrderedDict([\n",
    "                (\"basic1\", D_basic_layer(ndf, ndf*2, kernel_size=1, stride=1, padding=0))\n",
    "            ]))\n",
    "            self.final = nn.Conv2d(ndf*2, 1, kernel_size=1, stride=1, padding=0)\n",
    "        # Else 16x16 (n_layer=1), 34x34 (n_layer=2), 70x70 (n_layer=3), 142x142 (n_layer=4), 286x286 (n_layer=5)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_nc+out_nc, ndf, kernel_size=4, stride=2, padding=1)\n",
    "            self.main  = nn.Sequential(OrderedDict(\n",
    "                [(\"basic\" + str(i+1), D_basic_layer(ndf*2**i, ndf*2**(i+1)) if i<3\n",
    "                     else D_basic_layer(ndf*8, ndf*8)) for i in range(n_layer-1)] +\n",
    "                [(\"basic\" + str(n_layer), D_basic_layer(ndf*min(2**(n_layer-1),8), ndf*min(2**(n_layer),8), stride=1))\n",
    "            ]))\n",
    "            self.final = nn.Conv2d(ndf*min(2**(n_layer),8), 1, kernel_size=4, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, input, label):\n",
    "        input = torch.cat([input, label], dim=1)\n",
    "        input = F.leaky_relu(self.conv1(input), negative_slope=0.2, inplace=True)\n",
    "        input = self.main(input)\n",
    "        return torch.sigmoid(self.final(input))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.normal_(m.weight, mean=1.0, std=0.02)\n",
    "        nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compact function to get optimizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_pix2pix(in_nc, out_nc, ngf, ndf, device, lr = 0.0002, beta1 = 0.5):\n",
    "    model_G = G_unet(in_nc, out_nc, ngf).to(device)\n",
    "    model_D = D_basic(in_nc, out_nc, ndf).to(device)\n",
    "    # Init weights\n",
    "    [m.apply(weights_init) for m in (model_G, model_D)]\n",
    "    # Set starting epoch to 0 as default\n",
    "    epoch_start    = 0\n",
    "    # Init saved losses\n",
    "    saved_losses_G = []\n",
    "    saved_losses_D = []\n",
    "        \n",
    "    # Check for pretrained Model\n",
    "    if os.path.isfile('./saved_models/pix2pix_'+use_dataset+direction+'_saved_model.tar'):\n",
    "        pretrained = \"Users_answer\"\n",
    "        while pretrained not in [\"y\",\"n\"]:\n",
    "            pretrained = input(\"Pretrained Model available, use it? [y/n]:\")\n",
    "        # If User says \"y\", load weights\n",
    "        if pretrained==\"y\":\n",
    "            # Load data\n",
    "            saved_data = torch.load('./saved_models/pix2pix_'+use_dataset+direction+'_saved_model.tar', map_location=device)\n",
    "            # Transmit data\n",
    "            model_G.load_state_dict(saved_data['G_state_dict'])\n",
    "            model_D.load_state_dict(saved_data['D_state_dict'])\n",
    "            epoch_start    = saved_data['current_epoch']\n",
    "            saved_losses_G = saved_data['losses_G']\n",
    "            saved_losses_D = saved_data['losses_D']\n",
    "            \n",
    "    # Define Adam optimizer\n",
    "    opt_G = optim.Adam(model_G.parameters(), lr=lr, betas=(beta1,0.999))\n",
    "    opt_D = optim.Adam(model_D.parameters(), lr=lr, betas=(beta1,0.999))\n",
    "    \n",
    "    return model_G, model_D, opt_G, opt_D, epoch_start, (saved_losses_G, saved_losses_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, subfolder='train', direction='AtoB', resize=None, crop_size=None,\n",
    "                 h_flip=None, v_flip=None, rotate_angle=None, jitter=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to folders for train, val amd test\n",
    "            subfolder (str): Folder with images\n",
    "            direction (str) : Either 'AtoB' or 'BtoA' indicating which direction the prediction should go\n",
    "            resize (int or tuple): If int image will be resized to square otherwise to tuple size\n",
    "            crop_size (int) : Square random crop\n",
    "            v_flip (bool) : Perform random vertical flip with p=0.5\n",
    "            h_flip (bool) : Perform random horizontal flip with p=0.5\n",
    "            rotate_degree (int): Random rotate degree between (-value,value)\n",
    "            jitter (bool) : For randomly change brightness, contrast, saturation and hue\n",
    "            transform (torchvision obj) : Other transformation that have no randomization in it\n",
    "        \"\"\"\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.input_path      = os.path.join(image_dir, subfolder)\n",
    "        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n",
    "        self.direction       = direction\n",
    "        self.resize          = resize\n",
    "        self.crop_size       = crop_size\n",
    "        self.h_flip          = h_flip\n",
    "        self.v_flip          = v_flip\n",
    "        self.rotate_angle    = rotate_angle\n",
    "        self.jitter          = jitter\n",
    "        self.transform       = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img = Image.open(img_fn)\n",
    "        if self.direction == 'AtoB':\n",
    "            input  = img.crop((0, 0, img.width // 2, img.height))\n",
    "            target = img.crop((img.width // 2, 0, img.width, img.height))\n",
    "        elif self.direction == 'BtoA':\n",
    "            input  = img.crop((img.width // 2, 0, img.width, img.height))\n",
    "            target = img.crop((0, 0, img.width // 2, img.height))\n",
    "\n",
    "        # preprocessing\n",
    "        if self.resize:\n",
    "            if isinstance(self.resize, int):\n",
    "                input  = input.resize((self.resize, self.resize), Image.BILINEAR)\n",
    "                target = target.resize((self.resize, self.resize), Image.BILINEAR)\n",
    "            else:\n",
    "                input  = input.resize(self.resize[::-1], Image.BILINEAR)\n",
    "                target = target.resize(self.resize[::-1], Image.BILINEAR)\n",
    "        \n",
    "        if self.h_flip:\n",
    "            if random.random() < 0.5:\n",
    "                input  = input.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                target = target.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        if self.v_flip:\n",
    "            if random.random() < 0.5:\n",
    "                input  = input.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                target = target.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                \n",
    "        if self.rotate_angle:\n",
    "            rnd_angle = random.randint(-self.rotate_angle, self.rotate_angle)\n",
    "            input  = input.rotate(rnd_angle, expand=True)\n",
    "            target = target.rotate(rnd_angle, expand=True)\n",
    "        \n",
    "        if self.jitter:\n",
    "            target = transforms.functional.adjust_brightness(input, random.uniform(0.5,1.5))\n",
    "            target = transforms.functional.adjust_contrast(input, random.uniform(0.75,1.25))\n",
    "            target = transforms.functional.adjust_saturation(input, random.uniform(0.5,1.5))\n",
    "            target = transforms.functional.adjust_hue(input, random.uniform(-0.05,0.05))\n",
    "        \n",
    "        if self.crop_size:\n",
    "            # Get Random initial x and y coordinate\n",
    "            x = random.randint(0, self.resize - self.crop_size + 1)\n",
    "            \n",
    "            y = random.randint(0, self.resize - self.crop_size + 1)\n",
    "            # Crop\n",
    "            input  = input.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "            target = target.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "            \n",
    "        if self.transform is not None:\n",
    "            input  = self.transform(input)\n",
    "            target = self.transform(target)\n",
    "        \n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\n",
    "\n",
    "# Train data\n",
    "train_ds = DatasetFromFolder('./data/'+use_dataset+'/', subfolder='train', resize=286, crop_size=256, \n",
    "                             direction=direction, h_flip=True, transform=transform)\n",
    "\n",
    "train_dl = data.DataLoader(dataset=train_ds, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pix2pix(epochs, model_G, model_D, opt_G, opt_D, train_dl, device, lamb = 100,\n",
    "                epoch_start=0, saved_losses=([],[]), show_iter=200):\n",
    "    \n",
    "    # Check if start_epoch is greater than number of epochs, stop if the case\n",
    "    if epoch_start>epochs:\n",
    "        return\n",
    "    \n",
    "    # Extract saved losses\n",
    "    saved_losses_G, saved_losses_D = saved_losses\n",
    "    \n",
    "    # Define Losses\n",
    "    BCE_Loss = nn.BCELoss()\n",
    "    L1_loss  = nn.L1Loss()\n",
    "    \n",
    "    # Take time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epoch_start, epochs):\n",
    "        ## Try out lambda decaying\n",
    "        #if epoch%5==0 and epoch!=0:\n",
    "        #    lamb *= 0.8\n",
    "        #    print(\"Epoch %d: Decaying lambda to %.1f\" %(epoch, lamb))\n",
    "        \n",
    "        # Start Training Loop\n",
    "        for i, (input, label) in enumerate(train_dl):\n",
    "            # push images to device\n",
    "            x,y = input.to(device), label.to(device)\n",
    "        \n",
    "            ### Discriminator Training ###\n",
    "            model_D.zero_grad()\n",
    "            # Real pictures\n",
    "            out_real = model_D(x,y).squeeze()\n",
    "            lossD_real = BCE_Loss(out_real, torch.ones(out_real.size(), device=device))\n",
    "        \n",
    "            # Fake pictures \n",
    "            out_fake_D = model_D(x, model_G(x).detach()).squeeze()\n",
    "            # Loss function with all zeros for being fake\n",
    "            lossD_fake = BCE_Loss(out_fake_D, torch.zeros(out_fake_D.size(), device=device))\n",
    "            \n",
    "            lossD = (lossD_real + lossD_fake)/2\n",
    "            lossD.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            ### Generator Training ###\n",
    "            model_G.zero_grad()\n",
    "            gen_img = model_G(x)\n",
    "            out_fake_G = model_D(x, gen_img).squeeze()\n",
    "            # BCE Loss with all ones for simplitic training (see DCGAN)\n",
    "            BCE_lossG = BCE_Loss(out_fake_G, torch.ones(out_fake_G.size(), device=device))\n",
    "            # L1_loss\n",
    "            L1_lossG  = L1_loss(gen_img, y)\n",
    "            # Combine with user specified lambda\n",
    "            lossG = BCE_lossG + lamb*L1_lossG\n",
    "            lossG.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            # Save lossos\n",
    "            ## Discriminator\n",
    "            saved_losses_D.append(lossD.clone().detach())\n",
    "            ## Generator\n",
    "            saved_losses_G.append(lossG.clone().detach())\n",
    "            \n",
    "            # Show some Optimazation metrics\n",
    "            if (i+epoch*len(train_dl))%show_iter == 0:\n",
    "                # get time values\n",
    "                hours, rem = divmod(time.time()-start_time, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                print('({:0>2}:{:0>2}:{:0>2}) [{}/{}][{}/{}] -> {:.2f}%\\tLoss_D: {:.4f}, D(x): {:.4f}\\tLoss_G: {:.4f}, D(G(z)): {:.4f}'.format(\n",
    "                    int(hours),int(minutes), int(seconds), epoch, epochs, i, len(train_dl),\n",
    "                    100*(i+epoch*len(train_dl))/(epochs*len(train_dl)), lossD, out_real.mean(),\n",
    "                    lossG, out_fake_D.mean()))\n",
    "                \n",
    "            # Show currently inputs and Generated \n",
    "            if (i+epoch*len(train_dl))%(2*show_iter) == 0:\n",
    "                inputs  = x.cpu()\n",
    "                targets = y.cpu()\n",
    "                fakes   = model_G(x).detach().cpu()\n",
    "                img_tmp = torch.cat([inputs, targets, fakes], dim=2)\n",
    "                plt.figure(figsize=(10,10))\n",
    "                plt.axis(\"off\")\n",
    "                plt.imshow(np.transpose(vutils.make_grid(img_tmp[:4], padding=2, normalize=True),(1,2,0)))\n",
    "                plt.pause(0.001)\n",
    "                \n",
    "        ###############\n",
    "        # save current state, epoch and saved losses\n",
    "        torch.save({'G_state_dict': model_G.state_dict(),\n",
    "                    'D_state_dict': model_D.state_dict(),\n",
    "                    'current_epoch': epoch+1,\n",
    "                    'losses_G': saved_losses_G,\n",
    "                    'losses_D': saved_losses_D,\n",
    "                   },'./saved_models/pix2pix_'+use_dataset+direction+'_saved_model.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initialized models and optimizers\n",
    "model_G, model_D, opt_G, opt_D, epoch_start, saved_losses = get_pix2pix(in_nc=3,\n",
    "                                                                        out_nc=3,\n",
    "                                                                        ngf=64,\n",
    "                                                                        ndf=64,\n",
    "                                                                        device=device,\n",
    "                                                                        lr = 0.0002,\n",
    "                                                                        beta1 = 0.5)\n",
    "# Fit pix2pix\n",
    "fit_pix2pix(200, model_G, model_D, opt_G, opt_D, train_dl, device, 100,\n",
    "            epoch_start, saved_losses, show_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Losses\n",
    "model_G, model_D, opt_G, opt_D, epoch_start, saved_losses = get_pix2pix(in_nc=3,\n",
    "                                                                        out_nc=3,\n",
    "                                                                        ngf=64,\n",
    "                                                                        ndf=64,\n",
    "                                                                        device=device,\n",
    "                                                                        lr = 0.0002,\n",
    "                                                                        beta1 = 0.5)\n",
    "# Transfor to numpy\n",
    "np_loss_G = np.array([float(x.clone().detach()) for x in saved_losses[0]])\n",
    "np_loss_D = np.array([float(x.clone().detach()) for x in saved_losses[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, lags=10):\n",
    "    # init\n",
    "    ma_fi = np.zeros(lags)\n",
    "    ma    = np.copy(x[lags:])\n",
    "    for lag in range(lags):\n",
    "        # fade in\n",
    "        ma_fi += np.concatenate((np.zeros(lag), x[:lags-lag]))\n",
    "        # actual moving average\n",
    "        ma += x[lags-lag-1:-lag-1]\n",
    "    out = np.concatenate((ma_fi/range(1,lags+1), ma/lags))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_loss_G = moving_average(np_loss_G, len(train_dl))\n",
    "ma_loss_D = moving_average(np_loss_D, len(train_dl))\n",
    "np_losses = np.column_stack((np_loss_G, np_loss_D, ma_loss_G, ma_loss_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "## 'python -m visdom.server' into terminal\n",
    "from visdom import Visdom\n",
    "\n",
    "viz = Visdom()\n",
    "\n",
    "viz.line(\n",
    "    Y = np_losses,\n",
    "    X = np.repeat(np.arange(len(np_loss_G))/float(len(train_dl)), 4).reshape(-1,4),\n",
    "    opts = dict(xlabel='epoch',\n",
    "                ylabel='Loss',\n",
    "                title='training loss DCGAN',\n",
    "                legend=[\"Generator\",\n",
    "                        \"Discriminator\",\n",
    "                        \"Generator Moving Average\",\n",
    "                        \"Discriminator Moving Average\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Test set\n",
    "test_ds = DatasetFromFolder('./data/'+use_dataset+'/', subfolder='val', resize=286, crop_size=256, \n",
    "                             direction=direction, h_flip=True, transform=transform)\n",
    "\n",
    "test_dl = data.DataLoader(dataset=test_ds, batch_size=5, shuffle=True)\n",
    "\n",
    "X_test, Y_test = next(iter(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some training images\n",
    "fakes_test = model_G(X_test).detach().cpu()\n",
    "img_tmp = torch.cat([X_test, Y_test, fakes_test], dim=2)\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(img_tmp, padding=2, normalize=True),(1,2,0)))\n",
    "plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
